# -*- coding: utf-8 -*-
"""crawling twitter reply

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E2vALkot-2tZ3F7MEZ2x3oeq7tOnMXKA
"""

# Import required Python package
!pip install pandas

# Install Node.js (because tweet-harvest built using Node.js)
!sudo apt-get update
!sudo apt-get install -y ca-certificates curl gnupg
!sudo mkdir -p /etc/apt/keyrings
!curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg

!NODE_MAJOR=20 && echo "deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main" | sudo tee /etc/apt/sources.list.d/nodesource.list

!sudo apt-get update
!sudo apt-get install nodejs -y

!node -v

token_twitter= 'e26313c0df8d5bf08dd3c6c5a2d3ae512929174c' #'201c6ba49f493eb6c439d1820da737d0f2a12d63'

# Crawling data tweet

file= 'bea_cukai2023(23).csv'
katakunci= 'bea cukai lang:id since:2023-09-01 until:2023-09-13'
limit =1000

!npx -y tweet-harvest@latest -o "{file}" -s "{katakunci}" --tab "LATEST" -l {limit} --token {token_twitter}

import pandas as pd

# Specify the path to your CSV file
file_path = f"tweets-data/{file}"

# Read the CSV file into a pandas DataFrame
df = pd.read_csv(file_path)

# Display the DataFrame
display(df)

# crawlingt data reply

# files='twitter_data.csv'
# twitter_thread='https://x.com/AnakLolina2/status/1789107927145652376'
# limit='10'

# !npx --yes tweet-harvest@latest -o {files} -l {limit} --token {token_twitter} --thread {twitter_thread}

num_twt=len(df)
print(f'jumlah tweet={num_twt}.')